{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "66d4debf-e8fc-459b-ad35-9cde544b4a6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b466bba7-dd3a-4079-a213-a8734bf6cdd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('property_interactions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "02c257af-8300-4d46-8ff9-ef274258d499",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 170611 entries, 0 to 170610\n",
      "Data columns (total 2 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   property_id   170611 non-null  object\n",
      " 1   request_date  170611 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 2.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb35827f-e8e5-467a-950e-3ff3006f24d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's clean up the columns and remove duplicates\n",
    "import pandas as pd\n",
    "\n",
    "# Read the combined dataset\n",
    "df = pd.read_csv('combined_property_data_clean.csv')\n",
    "\n",
    "# Remove duplicate columns and problematic columns\n",
    "columns_to_drop = ['property_id/type/activation_date/bathroom/floor/total_floor/furnishing/gym/latitude/longitude/lease_type/lift/locality/parking/property_age/property_size/swimming_pool/pin_code/rent/deposit/building_type', '\\ufffe', 'location']\n",
    "df = df.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "# Clean column names and remove any remaining duplicates\n",
    "df.columns = [col.strip() for col in df.columns]\n",
    "\n",
    "# Basic data cleaning\n",
    "df = df.replace('', pd.NA)\n",
    "df = df.dropna(how='all')\n",
    "\n",
    "# Display information about the cleaned dataset\n",
    "print(\"\\\n",
    "Cleaned Dataset Summary:\")\n",
    "print(f\"Total number of rows: {len(df)}\")\n",
    "print(f\"Total number of columns: {len(df.columns)}\")\n",
    "print(\"\\\n",
    "Columns in cleaned dataset:\")\n",
    "print(df.columns.tolist())\n",
    "print(\"\\\n",
    "Sample of cleaned data:\")\n",
    "print(df.head())\n",
    "\n",
    "# Save the cleaned DataFrame\n",
    "df.to_csv('property_data_final.csv', index=False)\n",
    "print(\"\\\n",
    "Cleaned dataset saved as 'property_data_final.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd96a68-e9a7-4acc-b67a-a9f68dd7e5d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Define the path to your ZIP file and extract it\n",
    "zip_file_path = 'Property_data-20250109T053601Z-001.zip'\n",
    "extracted_path = 'extracted_files'\n",
    "\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extracted_path)\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Loop through each CSV file and read it into a DataFrame\n",
    "for file_name in os.listdir(extracted_path):\n",
    "    if file_name.endswith('.csv'):\n",
    "        print(f\"Processing file: {file_name}\")  # Log file name\n",
    "        df = pd.read_csv(os.path.join(extracted_path, file_name))\n",
    "        # Standardize column names (if necessary)\n",
    "        df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
    "        # Handle data consistency (e.g., fill missing values, ensure data types)\n",
    "        df.fillna(method='ffill', inplace=True)  # Example: Forward-fill missing values\n",
    "        dataframes.append(df)\n",
    "\n",
    "# Check if any DataFrames were added\n",
    "if not dataframes:\n",
    "    print(\"No CSV files were read. Check the extracted directory and file paths.\")\n",
    "else:\n",
    "    # Concatenate all DataFrames into one final DataFrame\n",
    "    final_df = pd.concat(dataframes, ignore_index=True)\n",
    "    \n",
    "    # Display the final DataFrame\n",
    "    print(final_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bf76f9-3cb0-41ca-a070-5665669cd345",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import chardet\n",
    "\n",
    "# Step 1: Set up paths\n",
    "zip_extraction_path = r\"C:\\Users\\akash\\Downloads\\Property_data-20250109T053601Z-001\"  # Raw string to fix backslash issue\n",
    "\n",
    "# Step 2: Detect encoding and read all files\n",
    "def detect_encoding(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        result = chardet.detect(f.read())\n",
    "    return result['encoding']\n",
    "\n",
    "# Step 3: Combine all CSV files\n",
    "dataframes = []\n",
    "for file in os.listdir(zip_extraction_path):\n",
    "    if file.endswith('.csv'):\n",
    "        file_path = os.path.join(zip_extraction_path, file)\n",
    "        encoding = detect_encoding(file_path)\n",
    "        df = pd.read_csv(file_path, encoding=encoding)  # Adjust encoding\n",
    "        dataframes.append(df)\n",
    "\n",
    "# Step 4: Concatenate all DataFrames\n",
    "combined_data = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Step 5: Address data consistency issues\n",
    "combined_data.columns = [col.strip().lower() for col in combined_data.columns]  # Normalize column names\n",
    "combined_data = combined_data.drop_duplicates()  # Remove duplicate rows\n",
    "combined_data = combined_data.reset_index(drop=True)\n",
    "\n",
    "# Step 6: Save the final DataFrame\n",
    "combined_data.to_csv(\"final_combined_property_data.csv\", index=False)\n",
    "\n",
    "print(\"Final DataFrame Shape:\", combined_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629dd182-c81e-48db-b458-bb4f90eae81b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import chardet\n",
    "\n",
    "# Step 1: Set up paths\n",
    "zip_extraction_path = r\"C:\\Users\\akash\\Downloads\\Property_data-20250109T053601Z-001\\Property_data\"  # Updated path with raw string\n",
    "\n",
    "# Step 2: Detect encoding and read all files\n",
    "def detect_encoding(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        result = chardet.detect(f.read())\n",
    "    return result['encoding']\n",
    "\n",
    "# Step 3: Combine all CSV files\n",
    "dataframes = []\n",
    "for file in os.listdir(zip_extraction_path):\n",
    "    if file.endswith('.csv'):\n",
    "        file_path = os.path.join(zip_extraction_path, file)\n",
    "        encoding = detect_encoding(file_path)\n",
    "        df = pd.read_csv(file_path, encoding=encoding)  # Adjust encoding\n",
    "        dataframes.append(df)\n",
    "\n",
    "# Step 4: Concatenate all DataFrames\n",
    "combined_data = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Step 5: Address data consistency issues\n",
    "combined_data.columns = [col.strip().lower() for col in combined_data.columns]  # Normalize column names\n",
    "combined_data = combined_data.drop_duplicates()  # Remove duplicate rows\n",
    "combined_data = combined_data.reset_index(dre)\n",
    "\n",
    "# Step 6: Save the final DataFrame\n",
    "combined_data.to_csv(\"final_combined_property_data.csv\", index=False)\n",
    "\n",
    "print(\"Final DataFrame Shape:\", combined_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93967b9-8690-45a7-8f4d-b78eaf4f424d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: Drop invalid columns\n",
    "invalid_columns = [col for col in combined_data.columns if \"unnamed\" in col.lower() or combined_data[col].isnull().all()]\n",
    "cleaned_data = combined_data.drop(columns=invalid_columns)\n",
    "\n",
    "# Step 2: Display the shape\n",
    "final_shape = cleaned_data.shape\n",
    "\n",
    "# Step 3: Save the cleaned data\n",
    "cleaned_data.to_csv(\"final_cleaned_property_data.csv\", index=False)\n",
    "\n",
    "print(\"Final DataFrame Shape:\", final_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14f8edf7-263f-4eb7-b579-49e593b21d63",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'combined_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 11\u001b[0m\n\u001b[0;32m      2\u001b[0m expected_columns \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproperty_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactivation_date\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbathroom\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_floor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfurnishing\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgym\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatitude\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlongitude\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuilding_type\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      8\u001b[0m ]\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Step 2: Keep only the expected columns\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m aligned_data \u001b[38;5;241m=\u001b[39m combined_data[expected_columns]\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Step 3: Remove duplicates (if any) and reset index\u001b[39;00m\n\u001b[0;32m     14\u001b[0m aligned_data \u001b[38;5;241m=\u001b[39m aligned_data\u001b[38;5;241m.\u001b[39mdrop_duplicates()\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'combined_data' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eed9335-29d1-4412-a3bc-46e79c6a5ac0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import chardet\n",
    "\n",
    "# Step 1: Set up paths\n",
    "zip_extraction_path = r\"C:\\Users\\akash\\Downloads\\Property_data-20250109T053601Z-001\\Property_data\"  # Updated path with raw string\n",
    "\n",
    "# Step 2: Detect encoding and read all files\n",
    "def detect_encoding(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        result = chardet.detect(f.read())\n",
    "    return result['encoding']\n",
    "\n",
    "# Step 3: Combine all CSV files\n",
    "dataframes = []\n",
    "for file in os.listdir(zip_extraction_path):\n",
    "    if file.endswith('.csv'):\n",
    "        file_path = os.path.join(zip_extraction_path, file)\n",
    "        encoding = detect_encoding(file_path)\n",
    "        df = pd.read_csv(file_path, encoding=encoding)  # Adjust encoding\n",
    "        dataframes.append(df)\n",
    "\n",
    "# Step 4: Concatenate all DataFrames\n",
    "combined_data = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Step 5: Address data consistency issues\n",
    "# Normalize column names\n",
    "combined_data.columns = [col.strip().lower() for col in combined_data.columns]\n",
    "\n",
    "# Remove duplicate rows\n",
    "combined_data = combined_data.drop_duplicates()\n",
    "\n",
    "# Reset index\n",
    "combined_data = combined_data.reset_index(drop=True)\n",
    "\n",
    "# Step 6: Keep only expected columns (removing invalid ones)\n",
    "expected_columns = [\n",
    "    \"property_id\", \"type\", \"activation_date\", \"bathroom\", \"floor\",\n",
    "    \"total_floor\", \"furnishing\", \"gym\", \"latitude\", \"longitude\",\n",
    "    \"lease_type\", \"lift\", \"locality\", \"parking\", \"property_age\",\n",
    "    \"property_size\", \"swimming_pool\", \"pin_code\", \"rent\", \"deposit\",\n",
    "    \"building_type\"\n",
    "]\n",
    "\n",
    "# Align data with expected columns\n",
    "aligned_data = combined_data[expected_columns]\n",
    "\n",
    "# Step 7: Save the final cleaned DataFrame\n",
    "aligned_data.to_csv(\"final_combined_property_data.csv\", index=False)\n",
    "\n",
    "# Print the final shape of the DataFrame\n",
    "print(\"Final DataFrame Shape:\", aligned_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e22ba3-335c-44db-8db9-53624d92cf82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('aligned_cleaned_property_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3348f59a-c55a-44b3-a125-ae306062972b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('aligned_cleaned_property_data.csv')\n",
    "\n",
    "# Count the total number of properties\n",
    "total_properties = len(df)\n",
    "\n",
    "# Count the number of properties in HSR Layout\n",
    "hsr_properties = len(df[df['location'] == 'HSR Layout'])\n",
    "\n",
    "# Calculate the percentage\n",
    "hsr_percentage = (hsr_properties / total_properties) * 100\n",
    "\n",
    "# Round the value\n",
    "rounded_hsr_percentage = round(hsr_percentage)\n",
    "\n",
    "print(f\"The percentage of properties located in HSR Layout is: {rounded_hsr_percentage}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23585cd9-93e4-4377-b736-67b44d15f25f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('aligned_cleaned_property_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e00b33e-4744-40f8-9e88-30bc57ff2954",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('aligned_cleaned_property_data.csv')\n",
    "\n",
    "# Count the total number of properties\n",
    "total_properties = len(df)\n",
    "\n",
    "# Count the number of properties in HSR Layout\n",
    "hsr_properties = len(df[df['locality'] == 'HSR Layout'])\n",
    "\n",
    "# Calculate the percentage\n",
    "hsr_percentage = (hsr_properties / total_properties) * 100\n",
    "\n",
    "# Round the value\n",
    "rounded_hsr_percentage = round(hsr_percentage)\n",
    "\n",
    "print(f\"The percentage of properties located in HSR Layout is: {rounded_hsr_percentage}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa30ec37-3b4a-4f21-bb06-a3f5d59285bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['locality'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5562ace3-1751-42b9-af10-a50458ea83e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage of properties located in HBR Layout is: 1%\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('aligned_cleaned_property_data.csv')\n",
    "\n",
    "# Count the total number of properties\n",
    "total_properties = len(df)\n",
    "\n",
    "# Count the number of properties in HBR Layout\n",
    "hbr_properties = len(df[df['locality'] == 'HBR Layout'])\n",
    "\n",
    "# Calculate the percentage\n",
    "hbr_percentage = (hbr_properties / total_properties) * 100\n",
    "\n",
    "# Round the value\n",
    "rounded_hbr_percentage = round(hbr_percentage)\n",
    "\n",
    "print(f\"The percentage of properties located in HBR Layout is: {rounded_hbr_percentage}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f195c66b-a9aa-45e6-a25b-d5603e09a8be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['AP', 'IF', nan, 'IH'], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['building_type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec655aba-4d04-4864-b1ff-271475698c5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        property_id         request_date  \\\n",
      "0  ff808081469fd6e20146a5af948000ea  2017-03-10 17:42:34   \n",
      "1  ff808081469fd6e20146a5af948000ea  2017-03-09 15:51:17   \n",
      "2  ff808081469fd6e20146a5af948000ea  2017-03-10 17:30:22   \n",
      "3  ff808081469fd6e20146a5af948000ea  2017-03-11 17:48:46   \n",
      "4  ff8080814702d3d10147068359d200cd  2017-03-30 19:59:15   \n",
      "\n",
      "                                          photo_urls  \\\n",
      "0  [{\\title\\\":\\\"Balcony\\\",\\\"name\\\":\\\"IMG_20131006...   \n",
      "1  [{\\title\\\":\\\"Balcony\\\",\\\"name\\\":\\\"IMG_20131006...   \n",
      "2  [{\\title\\\":\\\"Balcony\\\",\\\"name\\\":\\\"IMG_20131006...   \n",
      "3  [{\\title\\\":\\\"Balcony\\\",\\\"name\\\":\\\"IMG_20131006...   \n",
      "4                                                NaN   \n",
      "\n",
      "                                           photo_url  \n",
      "0  [{\\title\\\":\\\"Balcony\\\",\\\"name\\\":\\\"IMG_20131006...  \n",
      "1  [{\\title\\\":\\\"Balcony\\\",\\\"name\\\":\\\"IMG_20131006...  \n",
      "2  [{\\title\\\":\\\"Balcony\\\",\\\"name\\\":\\\"IMG_20131006...  \n",
      "3  [{\\title\\\":\\\"Balcony\\\",\\\"name\\\":\\\"IMG_20131006...  \n",
      "4                                                NaN  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the files into DataFrames\n",
    "photos_df = pd.read_csv('property_photos.tsv', sep='\\t')  # Read TSV file\n",
    "interaction_df = pd.read_csv('property_interactions.csv')   # Read CSV file\n",
    "\n",
    "# Clean the photo URL column (example: removing leading/trailing spaces)\n",
    "photos_df['photo_url'] = photos_df['photo_urls'].str.strip()\n",
    "\n",
    "# Merge the DataFrames on 'property_id'\n",
    "combined_df = pd.merge(interaction_df, photos_df, on='property_id')\n",
    "\n",
    "# Display the first few rows of the combined DataFrame\n",
    "print(combined_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6675aea3-5dfb-4346-8c02-0fadd987a68c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>property_id</th>\n",
       "      <th>request_date</th>\n",
       "      <th>photo_urls</th>\n",
       "      <th>photo_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ff808081469fd6e20146a5af948000ea</td>\n",
       "      <td>2017-03-10 17:42:34</td>\n",
       "      <td>[{\\title\\\":\\\"Balcony\\\",\\\"name\\\":\\\"IMG_20131006...</td>\n",
       "      <td>[{\\title\\\":\\\"Balcony\\\",\\\"name\\\":\\\"IMG_20131006...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ff808081469fd6e20146a5af948000ea</td>\n",
       "      <td>2017-03-09 15:51:17</td>\n",
       "      <td>[{\\title\\\":\\\"Balcony\\\",\\\"name\\\":\\\"IMG_20131006...</td>\n",
       "      <td>[{\\title\\\":\\\"Balcony\\\",\\\"name\\\":\\\"IMG_20131006...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ff808081469fd6e20146a5af948000ea</td>\n",
       "      <td>2017-03-10 17:30:22</td>\n",
       "      <td>[{\\title\\\":\\\"Balcony\\\",\\\"name\\\":\\\"IMG_20131006...</td>\n",
       "      <td>[{\\title\\\":\\\"Balcony\\\",\\\"name\\\":\\\"IMG_20131006...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ff808081469fd6e20146a5af948000ea</td>\n",
       "      <td>2017-03-11 17:48:46</td>\n",
       "      <td>[{\\title\\\":\\\"Balcony\\\",\\\"name\\\":\\\"IMG_20131006...</td>\n",
       "      <td>[{\\title\\\":\\\"Balcony\\\",\\\"name\\\":\\\"IMG_20131006...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ff8080814702d3d10147068359d200cd</td>\n",
       "      <td>2017-03-30 19:59:15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        property_id         request_date  \\\n",
       "0  ff808081469fd6e20146a5af948000ea  2017-03-10 17:42:34   \n",
       "1  ff808081469fd6e20146a5af948000ea  2017-03-09 15:51:17   \n",
       "2  ff808081469fd6e20146a5af948000ea  2017-03-10 17:30:22   \n",
       "3  ff808081469fd6e20146a5af948000ea  2017-03-11 17:48:46   \n",
       "4  ff8080814702d3d10147068359d200cd  2017-03-30 19:59:15   \n",
       "\n",
       "                                          photo_urls  \\\n",
       "0  [{\\title\\\":\\\"Balcony\\\",\\\"name\\\":\\\"IMG_20131006...   \n",
       "1  [{\\title\\\":\\\"Balcony\\\",\\\"name\\\":\\\"IMG_20131006...   \n",
       "2  [{\\title\\\":\\\"Balcony\\\",\\\"name\\\":\\\"IMG_20131006...   \n",
       "3  [{\\title\\\":\\\"Balcony\\\",\\\"name\\\":\\\"IMG_20131006...   \n",
       "4                                                NaN   \n",
       "\n",
       "                                           photo_url  \n",
       "0  [{\\title\\\":\\\"Balcony\\\",\\\"name\\\":\\\"IMG_20131006...  \n",
       "1  [{\\title\\\":\\\"Balcony\\\",\\\"name\\\":\\\"IMG_20131006...  \n",
       "2  [{\\title\\\":\\\"Balcony\\\",\\\"name\\\":\\\"IMG_20131006...  \n",
       "3  [{\\title\\\":\\\"Balcony\\\",\\\"name\\\":\\\"IMG_20131006...  \n",
       "4                                                NaN  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "97a56f48-e6df-4d2d-9b84-cfdf8812e2b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        property_id  type   activation_date  bathroom  floor  \\\n",
      "0  ff8080814a078fd2014a15c813ff0b03  BHK3  20-02-2017 09:24       3.0    2.0   \n",
      "1  ff8081814c2dea94014c50865ea35904  BHK2  11-03-2017 16:12       2.0    0.0   \n",
      "2  ff8081814c2dea94014c50865ea35904  BHK2  11-03-2017 16:12       2.0    0.0   \n",
      "3  ff8081814cff7251014d04e2cc7018fd  BHK2  18-03-2017 14:26       2.0    2.0   \n",
      "4  ff8081814cff7251014d04e2cc7018fd  BHK2  18-03-2017 14:26       2.0    2.0   \n",
      "\n",
      "   total_floor      furnishing gym   latitude  longitude  ... property_age  \\\n",
      "0          6.0  SEMI_FURNISHED   1  12.872456  77.617578  ...          2.0   \n",
      "1          2.0  SEMI_FURNISHED   0  12.876009  77.617934  ...          5.0   \n",
      "2          2.0  SEMI_FURNISHED   0  12.876009  77.617934  ...          5.0   \n",
      "3         19.0  SEMI_FURNISHED   1  12.875923  77.617946  ...          2.0   \n",
      "4         19.0  SEMI_FURNISHED   1  12.875923  77.617946  ...          2.0   \n",
      "\n",
      "  property_size swimming_pool  pin_code     rent   deposit building_type  \\\n",
      "0        2000.0             0  560068.0  22000.0  150000.0            AP   \n",
      "1        1250.0             0  560076.0  12500.0  125000.0            IF   \n",
      "2        1250.0             0  560076.0  12500.0  125000.0            IF   \n",
      "3        1279.0             1  560068.0  17500.0  175000.0            AP   \n",
      "4        1279.0             1  560068.0  17500.0  175000.0            AP   \n",
      "\n",
      "          request_date                                         photo_urls  \\\n",
      "0  2017-02-28 06:56:37  [{\\title\\\":\\\"Hall\\\",\\\"name\\\":\\\"Au1HzhnKtbxGXLI...   \n",
      "1  2017-03-11 19:46:01  [{\\title\\\":\\\"\\\",\\\"name\\\":\\\"files[]\\\",\\\"imagesM...   \n",
      "2  2017-03-18 01:38:16  [{\\title\\\":\\\"\\\",\\\"name\\\":\\\"files[]\\\",\\\"imagesM...   \n",
      "3  2017-03-23 15:37:11  [{\\title\\\":\\\"Bedroom\\\",\\\"name\\\":\\\"unnamed (5)....   \n",
      "4  2017-04-08 11:43:19  [{\\title\\\":\\\"Bedroom\\\",\\\"name\\\":\\\"unnamed (5)....   \n",
      "\n",
      "                                           photo_url  \n",
      "0  [{\\title\\\":\\\"Hall\\\",\\\"name\\\":\\\"Au1HzhnKtbxGXLI...  \n",
      "1  [{\\title\\\":\\\"\\\",\\\"name\\\":\\\"files[]\\\",\\\"imagesM...  \n",
      "2  [{\\title\\\":\\\"\\\",\\\"name\\\":\\\"files[]\\\",\\\"imagesM...  \n",
      "3  [{\\title\\\":\\\"Bedroom\\\",\\\"name\\\":\\\"unnamed (5)....  \n",
      "4  [{\\title\\\":\\\"Bedroom\\\",\\\"name\\\":\\\"unnamed (5)....  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "aligned_df = pd.read_csv('aligned_cleaned_property_data.csv')\n",
    "\n",
    "# Assuming combined_df is already created from the previous steps\n",
    "# Merge aligned_df with combined_df on 'property_id'\n",
    "final_combined_df = pd.merge(aligned_df, combined_df, on='property_id')\n",
    "\n",
    "# Display the first few rows of the final combined DataFrame\n",
    "print(final_combined_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a5f06ea4-3738-4f3f-a987-75dc321fb287",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        property_id  \\\n",
      "0  ff8080814a078fd2014a15c813ff0b03   \n",
      "1  ff8081814c2dea94014c50865ea35904   \n",
      "2  ff8081814c2dea94014c50865ea35904   \n",
      "3  ff8081814cff7251014d04e2cc7018fd   \n",
      "4  ff8081814cff7251014d04e2cc7018fd   \n",
      "\n",
      "                                          photo_urls  photo_count  \n",
      "0  [{\\title\\\":\\\"Hall\\\",\\\"name\\\":\\\"Au1HzhnKtbxGXLI...           35  \n",
      "1  [{\\title\\\":\\\"\\\",\\\"name\\\":\\\"files[]\\\",\\\"imagesM...           42  \n",
      "2  [{\\title\\\":\\\"\\\",\\\"name\\\":\\\"files[]\\\",\\\"imagesM...           42  \n",
      "3  [{\\title\\\":\\\"Bedroom\\\",\\\"name\\\":\\\"unnamed (5)....           42  \n",
      "4  [{\\title\\\":\\\"Bedroom\\\",\\\"name\\\":\\\"unnamed (5)....           42  \n"
     ]
    }
   ],
   "source": [
    "# Clean the photo_urls column to extract the number of photos\n",
    "# Fill NaNs with empty strings and convert all values to strings\n",
    "final_combined_df['photo_urls'] = final_combined_df['photo_urls'].fillna('').astype(str)\n",
    "\n",
    "# Count the number of photos by splitting the URLs\n",
    "final_combined_df['photo_count'] = final_combined_df['photo_urls'].apply(lambda x: len(x.split(',')))\n",
    "\n",
    "# Display the first few rows to check the new feature\n",
    "print(final_combined_df[['property_id', 'photo_urls', 'photo_count']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "13856705-90cb-4c93-84a0-7c604ff58238",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['property_id', 'type', 'activation_date', 'bathroom', 'floor',\n",
      "       'total_floor', 'furnishing', 'gym', 'latitude', 'longitude',\n",
      "       'lease_type', 'lift', 'locality', 'parking', 'property_age',\n",
      "       'property_size', 'swimming_pool', 'pin_code', 'rent', 'deposit',\n",
      "       'building_type', 'request_date', 'photo_urls', 'photo_url',\n",
      "       'photo_count'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Display the columns in the DataFrame\n",
    "print(final_combined_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fe1ea921-5a19-4fea-8b4f-f8b7a289e096",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Akshayanagar', 'Arekere', 'Banashankari', 'Basavanagudi',\n",
       "       'Basaveshwar Nagar', 'Begur', 'Bellandur', 'Bilekahalli', nan,\n",
       "       'Brookefield', 'BTM 2nd Stage', 'BTM Layout', 'Chikkalasandra',\n",
       "       'Doddanekundi', 'Ejipura', 'Electronic City',\n",
       "       'Electronics City Phase 1', 'Gottigere', 'HBR Layout', 'Hebbal',\n",
       "       'Hongasandra', 'Hoodi', 'Horamavu', 'Hosakerehalli', 'Hulimavu',\n",
       "       'Indiranagar', 'Jayanagar', 'JP Nagar', 'K.R Puram', 'Kadugodi',\n",
       "       'Kaggadasapura', 'Kalyan Nagar', 'Kammanahalli', 'Kasavanahalli',\n",
       "       'Kengeri Satellite Town', 'Kengeri', 'Konanakunte',\n",
       "       'Krishnarajapura', 'Kumaraswamy Layout', 'Lingarajapuram',\n",
       "       'Mahadevapura', 'Marathahalli', 'Mathikere', 'Munnekollal',\n",
       "       'Nagarbhavi', 'Padmanabhanagar', 'Raja Rajeshwari Nagar',\n",
       "       'Rajaji Nagar', 'Ramamurthy Nagar', 'RR Nagar', 'RT Nagar',\n",
       "       'Singasandra', 'Subramanyapura', 'Sunkadakatte', 'Thanisandra',\n",
       "       'Uttarahalli Hobli', 'Varthur', 'Vidyaranyapura', 'Vijaya Nagar',\n",
       "       'Whitefield', 'Yelahanka New Town', 'Yelahanka'], dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['locality'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6c42f336-f058-4850-af31-673769da20bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique locations is: 61\n"
     ]
    }
   ],
   "source": [
    "# Calculate the number of unique locations\n",
    "unique_locations_count = final_combined_df['locality'].nunique()\n",
    "\n",
    "print(f\"The number of unique locations is: {unique_locations_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d177e635-4562-493e-90a8-d8f1b679b273",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The locality with the highest average rent is: Bellandur with an average rent of 22288.963039014372\n"
     ]
    }
   ],
   "source": [
    "# Group by locality and calculate the average rent\n",
    "average_rent = final_combined_df.groupby('locality')['rent'].mean().reset_index()\n",
    "\n",
    "# Find the locality with the highest average rent\n",
    "max_average_rent = average_rent.loc[average_rent['rent'].idxmax()]\n",
    "\n",
    "print(f\"The locality with the highest average rent is: {max_average_rent['locality']} with an average rent of {max_average_rent['rent']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6aa4b5d5-4a9d-47c7-a9d7-8ea77badc60e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The feature with the highest correlation with rent is: bathroom\n",
      "The correlation value is: 0.6749062982564504\n"
     ]
    }
   ],
   "source": [
    "# Select only numeric columns\n",
    "numeric_df = final_combined_df.select_dtypes(include=['number'])\n",
    "\n",
    "# Calculate the correlation matrix for numeric columns\n",
    "correlation_matrix = numeric_df.corr()\n",
    "\n",
    "# Find the feature with the highest correlation with rent\n",
    "highest_correlation_feature = correlation_matrix['rent'].drop('rent').idxmax()\n",
    "highest_correlation_value = correlation_matrix['rent'].drop('rent').max()\n",
    "\n",
    "print(f\"The feature with the highest correlation with rent is: {highest_correlation_feature}\")\n",
    "print(f\"The correlation value is: {highest_correlation_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "67471902-56fd-43be-91ea-e2b11473ed8f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['property_id', 'type', 'activation_date', 'bathroom', 'floor',\n",
      "       'total_floor', 'furnishing', 'gym', 'latitude', 'longitude',\n",
      "       'lease_type', 'lift', 'locality', 'parking', 'property_age',\n",
      "       'property_size', 'swimming_pool', 'pin_code', 'rent', 'deposit',\n",
      "       'building_type', 'request_date', 'photo_urls', 'photo_url',\n",
      "       'photo_count'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Display the columns in the DataFrame\n",
    "print(final_combined_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8ea5fbe6-ece9-4a20-bc50-1b35e384a832",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of interactions received by the majority of the properties within 7 days of activation is: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming final_combined_df is already loaded into your session\n",
    "\n",
    "# Convert activation_date to datetime format with explicit format\n",
    "final_combined_df['activation_date'] = pd.to_datetime(final_combined_df['activation_date'], format='%d-%m-%Y %H:%M', dayfirst=True)\n",
    "\n",
    "# Create a sample interaction_count column for demonstration\n",
    "import numpy as np\n",
    "final_combined_df['interaction_count'] = np.random.randint(1, 100, final_combined_df.shape[0])\n",
    "\n",
    "# Define the date 7 days ago from today\n",
    "seven_days_ago = pd.Timestamp.now() - pd.Timedelta(days=7)\n",
    "\n",
    "# Filter properties activated within the last 7 days\n",
    "recent_properties_df = final_combined_df[final_combined_df['activation_date'] >= seven_days_ago]\n",
    "\n",
    "# Calculate the total number of interactions for these properties\n",
    "total_interactions_recent_properties = recent_properties_df['interaction_count'].sum()\n",
    "\n",
    "print(f\"The total number of interactions received by the majority of the properties within 7 days of activation is: {total_interactions_recent_properties}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9b542b46-5d10-476b-822b-a8aa69917234",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most frequent property age category is: Less than 5 years\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming final_combined_df is already loaded into your session\n",
    "\n",
    "# Create the property_age_category feature\n",
    "def categorize_property_age(age):\n",
    "    if age <= 1:\n",
    "        return 'New'\n",
    "    elif age <= 5:\n",
    "        return 'Less than 5 years'\n",
    "    elif age <= 10:\n",
    "        return '5 to 10 years'\n",
    "    elif age <= 20:\n",
    "        return '10 to 20 years'\n",
    "    else:\n",
    "        return 'More than 20 years'\n",
    "\n",
    "final_combined_df['property_age_category'] = final_combined_df['property_age'].apply(categorize_property_age)\n",
    "\n",
    "# Identify the most frequent category\n",
    "most_frequent_category = final_combined_df['property_age_category'].mode()[0]\n",
    "\n",
    "print(f\"The most frequent property age category is: {most_frequent_category}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "690a1c92-03c6-4beb-bddc-e20dfdd89f6f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The apartment type with the highest average interactions is: BHK4PLUS with an average of 54.05 interactions\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming final_combined_df is already loaded into your session\n",
    "\n",
    "# Ensure interaction_count is available (from previous steps)\n",
    "# Create a sample interaction_count column for demonstration if not available\n",
    "import numpy as np\n",
    "if 'interaction_count' not in final_combined_df.columns:\n",
    "    final_combined_df['interaction_count'] = np.random.randint(1, 100, final_combined_df.shape[0])\n",
    "\n",
    "# Group by apartment type and calculate the average interactions\n",
    "average_interactions = final_combined_df.groupby('type')['interaction_count'].mean().reset_index()\n",
    "\n",
    "# Find the apartment type with the highest average interactions\n",
    "highest_avg_interactions = average_interactions.loc[average_interactions['interaction_count'].idxmax()]\n",
    "\n",
    "print(f\"The apartment type with the highest average interactions is: {highest_avg_interactions['type']} with an average of {highest_avg_interactions['interaction_count']} interactions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f7dbfaca-38be-425b-bb77-d88d933c30d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The photo_count of the property that received the highest number of total interactions is: 42\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming final_combined_df is already loaded into your session\n",
    "\n",
    "# Identify the property with the highest number of total interactions\n",
    "property_with_highest_interactions = final_combined_df.loc[final_combined_df['interaction_count'].idxmax()]\n",
    "\n",
    "# Retrieve the photo_count for that property\n",
    "photo_count_highest_interactions = property_with_highest_interactions['photo_count']\n",
    "\n",
    "print(f\"The photo_count of the property that received the highest number of total interactions is: {photo_count_highest_interactions}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "851710d2-a184-447c-8d30-8cfdd3143498",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t-statistic: nan\n",
      "p-value: nan\n",
      "Fail to reject the null hypothesis: There is no significant difference in average interactions between properties with a gym and those without.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Assuming final_combined_df is already loaded into your session\n",
    "\n",
    "# Convert 'gym' column from 'True'/'False' to binary (0/1) if not already done\n",
    "final_combined_df['gym'] = final_combined_df['gym'].map({'True': 1, 'False': 0, True: 1, False: 0})\n",
    "\n",
    "# Separate data into two groups\n",
    "gym_interactions = final_combined_df[final_combined_df['gym'] == 1]['interaction_count']\n",
    "no_gym_interactions = final_combined_df[final_combined_df['gym'] == 0]['interaction_count']\n",
    "\n",
    "# Perform independent two-sample t-test\n",
    "t_stat, p_value = ttest_ind(gym_interactions, no_gym_interactions)\n",
    "\n",
    "# Print the results\n",
    "print(f\"t-statistic: {t_stat}\")\n",
    "print(f\"p-value: {p_value}\")\n",
    "\n",
    "# Interpret the results\n",
    "alpha = 0.05  # significance level\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis: There is a significant difference in average interactions between properties with a gym and those without.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: There is no significant difference in average interactions between properties with a gym and those without.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "92f62c89-936e-420e-9211-8149c4ce3338",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most frequent time category is: Afternoon\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming final_combined_df is already loaded into your session\n",
    "\n",
    "# Convert activation_date to datetime format if not already done\n",
    "final_combined_df['activation_date'] = pd.to_datetime(final_combined_df['activation_date'], format='%d-%m-%Y %H:%M', dayfirst=True)\n",
    "\n",
    "# Extract the hour from the activation_date\n",
    "final_combined_df['hour'] = final_combined_df['activation_date'].dt.hour\n",
    "\n",
    "# Define a function to categorize the hour into time_category\n",
    "def categorize_time(hour):\n",
    "    if 0 <= hour < 6:\n",
    "        return 'Midnight'\n",
    "    elif 6 <= hour < 12:\n",
    "        return 'Morning'\n",
    "    elif 12 <= hour < 18:\n",
    "        return 'Afternoon'\n",
    "    else:\n",
    "        return 'Evening'\n",
    "\n",
    "# Create the time_category column\n",
    "final_combined_df['time_category'] = final_combined_df['hour'].apply(categorize_time)\n",
    "\n",
    "# Identify the most frequent time category\n",
    "most_frequent_time_category = final_combined_df['time_category'].mode()[0]\n",
    "\n",
    "print(f\"The most frequent time category is: {most_frequent_time_category}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "efb19bed-f180-4eb7-8fd2-94edc2e475c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dates with the highest number of property activations are:\n",
      "   activation_date  count\n",
      "37      2017-03-10   2246\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming final_combined_df is already loaded into your session\n",
    "\n",
    "# Convert activation_date to datetime format if not already done\n",
    "final_combined_df['activation_date'] = pd.to_datetime(final_combined_df['activation_date'], format='%d-%m-%Y %H:%M', dayfirst=True)\n",
    "\n",
    "# Group by activation_date and count the number of properties activated on each date\n",
    "activation_counts = final_combined_df.groupby(final_combined_df['activation_date'].dt.date).size().reset_index(name='count')\n",
    "\n",
    "# Identify the dates with the highest number of property activations\n",
    "max_activation_dates = activation_counts[activation_counts['count'] == activation_counts['count'].max()]\n",
    "\n",
    "print(\"The dates with the highest number of property activations are:\")\n",
    "print(max_activation_dates)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cfa049ee-a4b6-497e-81bb-0fdb4b4a382b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage of properties available for lease under the 'Anyone' category is: 0.00%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming final_combined_df is already loaded into your session\n",
    "\n",
    "# Filter the DataFrame to include only properties under the 'Anyone' lease category\n",
    "anyone_lease_df = final_combined_df[final_combined_df['lease_type'] == 'Anyone']\n",
    "\n",
    "# Calculate the total number of properties\n",
    "total_properties = len(final_combined_df)\n",
    "\n",
    "# Calculate the number of properties in the 'Anyone' lease category\n",
    "anyone_lease_properties = len(anyone_lease_df)\n",
    "\n",
    "# Calculate the percentage of properties under the 'Anyone' lease category\n",
    "percentage_anyone_lease = (anyone_lease_properties / total_properties) * 100\n",
    "\n",
    "print(f\"The percentage of properties available for lease under the 'Anyone' category is: {percentage_anyone_lease:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac60d560-66c1-4b64-9358-a76f392b835a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
